{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0d3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets tqdm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2ae45",
   "metadata": {},
   "source": [
    "# BERTopic Analysis on CCNews Dataset\n",
    "\n",
    "**Dataset**: Stanford CCNews 2024 - A large-scale news dataset containing articles from Common Crawl, representing global news discourse across multiple languages and regions.\n",
    "\n",
    "**Methodology**: \n",
    "- **Topic Modeling**: BERTopic with multilingual embeddings to discover latent topics\n",
    "- **Embeddings**: E5-small model for semantic understanding across languages\n",
    "- **Clustering**: UMAP + HDBSCAN for topic discovery\n",
    "\n",
    "**Technical Setup**:\n",
    "- Sample: 500 multilingual articles for computational efficiency\n",
    "- Tools: BERTopic, sentence-transformers, UMAP, HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af920011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CCNews dataset...\n"
     ]
    }
   ],
   "source": [
    "import json, re, os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Model Parameters\n",
    "EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-small\"\n",
    "MIN_TOPIC_SIZE = 10       # Minimum number of documents to form a topic\n",
    "N_NEIGHBORS = 12          # Used by UMAP for dimensionality reduction\n",
    "N_COMPONENTS = 5          # Used by UMAP for dimensionality reduction\n",
    "RANDOM_STATE = 42         # Ensures reproducible results\n",
    "\n",
    "# Load dataset in streaming mode and convert to DataFrame\n",
    "print(\"Loading CCNews dataset...\")\n",
    "# Use num_proc=1 to avoid multiprocessing issues\n",
    "dataset = load_dataset(\"stanford-oval/ccnews\", name=\"2024\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "425a724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated a list with 354 English stopwords.\n"
     ]
    }
   ],
   "source": [
    "# English Stopwords (from M2_NLP_TopicModelBert)\n",
    "def get_english_stopwords():\n",
    "    \"\"\"Creates a robust list of English stopwords.\"\"\"\n",
    "    try:\n",
    "        # Start with the standard list from sklearn\n",
    "        from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_sw\n",
    "        base = set(sklearn_sw)\n",
    "    except ImportError:\n",
    "        # Fallback to a basic list if sklearn is not available\n",
    "        base = {\"a\", \"an\", \"and\", \"the\", \"in\", \"is\", \"it\", \"of\", \"for\", \"on\", \"with\", \"as\", \"by\", \"that\", \"this\"}\n",
    "\n",
    "    # Add custom stopwords common in news data\n",
    "    custom_stopwords = {\n",
    "        \"im\", \"ive\", \"id\", \"like\", \"just\", \"dont\", \"know\", \"feel\", \"think\",\n",
    "        \"people\", \"really\", \"want\", \"time\", \"would\", \"get\", \"one\", \"even\", \"go\", \"going\",\n",
    "        \"said\", \"say\", \"make\", \"something\", \"anything\", \"everything\",\n",
    "        \"news\", \"report\", \"reports\", \"according\", \"sources\", \"source\", \"article\",\n",
    "        \"today\", \"yesterday\", \"tomorrow\", \"week\", \"month\", \"year\", \"years\",\n",
    "        \"new\", \"latest\", \"breaking\", \"update\", \"updates\"\n",
    "    }\n",
    "    base.update(custom_stopwords)\n",
    "\n",
    "    return sorted(list(base))\n",
    "\n",
    "ENGLISH_STOPWORDS = get_english_stopwords()\n",
    "print(f\"Generated a list with {len(ENGLISH_STOPWORDS)} English stopwords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "828f164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting streaming dataset to DataFrame with 500 samples from all languages...\n",
      "Found 100 articles (processed 100 total)...\n",
      "Found 200 articles (processed 200 total)...\n",
      "Found 100 articles (processed 100 total)...\n",
      "Found 200 articles (processed 200 total)...\n",
      "Found 300 articles (processed 300 total)...\n",
      "Found 300 articles (processed 300 total)...\n",
      "Found 400 articles (processed 400 total)...\n",
      "Found 500 articles (processed 500 total)...\n",
      "Found 400 articles (processed 400 total)...\n",
      "Found 500 articles (processed 500 total)...\n",
      "Final: Found 500 articles after processing 500 total articles\n",
      "\n",
      "Language distribution:\n",
      "language\n",
      "en    182\n",
      "es    109\n",
      "ru     45\n",
      "ar     45\n",
      "pt     38\n",
      "fr     23\n",
      "kn     13\n",
      "it     12\n",
      "pl      8\n",
      "tr      7\n",
      "Name: count, dtype: int64\n",
      "Prepared 500 documents for analysis\n",
      "Final: Found 500 articles after processing 500 total articles\n",
      "\n",
      "Language distribution:\n",
      "language\n",
      "en    182\n",
      "es    109\n",
      "ru     45\n",
      "ar     45\n",
      "pt     38\n",
      "fr     23\n",
      "kn     13\n",
      "it     12\n",
      "pl      8\n",
      "tr      7\n",
      "Name: count, dtype: int64\n",
      "Prepared 500 documents for analysis\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Preparation (adapted from M2_NLP_TopicModelBert)\n",
    "def prepare_ccnews_data(dataset, sample_size=500):\n",
    "    \"\"\"Load and prepare CCNews data for topic modeling.\"\"\"\n",
    "    print(f\"Converting streaming dataset to DataFrame with {sample_size} samples from all languages...\")\n",
    "    \n",
    "    # Convert streaming dataset to list\n",
    "    data_list = []\n",
    "    count = 0\n",
    "    processed = 0\n",
    "    for item in dataset['train']:\n",
    "        processed += 1\n",
    "        # Include all articles regardless of language\n",
    "        data_list.append(item)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Found {count} articles (processed {processed} total)...\")\n",
    "        if count >= sample_size:\n",
    "            break\n",
    "    \n",
    "    print(f\"Final: Found {count} articles after processing {processed} total articles\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    # Normalize columns to avoid errors  \n",
    "    if 'title' not in df.columns:\n",
    "        df['title'] = \"\"\n",
    "    if 'text' not in df.columns:\n",
    "        df['text'] = \"\"\n",
    "    \n",
    "    df[\"title\"] = df[\"title\"].fillna(\"\").astype(str)\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n",
    "    \n",
    "    # Combine title and text for complete document (following M2 pattern)\n",
    "    df[\"full_text\"] = (df[\"title\"].str.strip() + \" . \" + df[\"text\"].str.strip()).str.strip()\n",
    "    \n",
    "    # Remove empty documents\n",
    "    df = df[df[\"full_text\"].str.len() > 0].reset_index(drop=True)\n",
    "    \n",
    "    # Show language distribution if language field exists\n",
    "    if 'language' in df.columns:\n",
    "        print(\"\\nLanguage distribution:\")\n",
    "        print(df['language'].value_counts().head(10))\n",
    "    \n",
    "    print(f\"Prepared {len(df)} documents for analysis\")\n",
    "    return df\n",
    "\n",
    "# Load and prepare the data\n",
    "df = prepare_ccnews_data(dataset, sample_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bc62626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of prepared data:\n",
      "                                               title  \\\n",
      "0  Anadolu Otoyolu'nun Kocaeli kesimindeki trafik...   \n",
      "1  Fallece madre de Michelle Obama, Marian Robins...   \n",
      "2  Nearly 1.9 million Fiji water bottles recalled...   \n",
      "\n",
      "                                           full_text  \\\n",
      "0  Anadolu Otoyolu'nun Kocaeli kesimindeki trafik...   \n",
      "1  Fallece madre de Michelle Obama, Marian Robins...   \n",
      "2  Nearly 1.9 million Fiji water bottles recalled...   \n",
      "\n",
      "                                          text_clean  \n",
      "0  anadolu otoyolu'nun kocaeli kesimindeki trafik...  \n",
      "1  fallece madre de michelle obama, marian robins...  \n",
      "2  nearly 1.9 million fiji water bottles recalled...  \n"
     ]
    }
   ],
   "source": [
    "# Text Cleaning for the Vectorizer (from M2_NLP_TopicModelBert)\n",
    "CLEAN_RE_URL = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "CLEAN_RE_WS  = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_for_vectorizer(s: str) -> str:\n",
    "    \"\"\"Clean text for the CountVectorizer.\"\"\"\n",
    "    s = s.lower()\n",
    "    s = CLEAN_RE_URL.sub(\" \", s)\n",
    "    s = CLEAN_RE_WS.sub(\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "# Apply text cleaning\n",
    "df[\"text_clean\"] = df[\"full_text\"].apply(clean_for_vectorizer)\n",
    "\n",
    "# Show sample of the data\n",
    "print(\"Sample of prepared data:\")\n",
    "print(df[['title', 'full_text', 'text_clean']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89661e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Generating embeddings... This may take a few minutes.\n",
      "Generating embeddings... This may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 16/16 [00:12<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (500, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating Text Embeddings (from M2_NLP_TopicModelBert)\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# E5 models work best with a specific prefix\n",
    "texts_for_embedding = [f\"passage: {t}\" for t in df[\"full_text\"].tolist()]\n",
    "\n",
    "print(\"Generating embeddings... This may take a few minutes.\")\n",
    "# Generate embeddings\n",
    "embeddings = embedding_model.encode(\n",
    "    texts_for_embedding,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(f\"Generated embeddings with shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a34e9612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All model components configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuring the BERTopic Model Components (from M2_NLP_TopicModelBert)\n",
    "\n",
    "# For keyword extraction\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=ENGLISH_STOPWORDS,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "# For dimensionality reduction\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=N_NEIGHBORS,\n",
    "    n_components=N_COMPONENTS,\n",
    "    metric=\"cosine\",\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# For clustering\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=MIN_TOPIC_SIZE,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# For topic representation (using KeyBERTInspired as in M2)\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "print(\"All model components configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb1ea18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: representation-ollama\n",
    "#| eval: false\n",
    "# To use this block, set `eval: true` in the line above and `eval: false` in the KeyBERT block.\n",
    "\n",
    "import openai\n",
    "from bertopic.representation import OpenAI\n",
    "\n",
    "# This prompt uses a \"few-shot\" technique to guide the LLM to generate high-quality topic names.\n",
    "prompt = \"\"\"\n",
    "I will provide you with sample texts and keywords from a topic. Your task is to create a concise, descriptive name (3-7 words) that accurately captures the topic's essence.\n",
    "Requirements:\n",
    "\n",
    "Use clear, specific language\n",
    "Focus on the core theme, not peripheral details\n",
    "Use natural phrasing (avoid generic words like \"issues\" or \"topics\")\n",
    "Be descriptive enough that someone unfamiliar with the content would understand the topic\n",
    "\n",
    "###EXAMPLES###\n",
    "Topic:\n",
    "Sample texts from this topic:\n",
    "\n",
    "I just started learning Python and I'm confused about when to use lists vs dictionaries.\n",
    "My code keeps throwing a 'KeyError' and I can't figure out why.\n",
    "What's the best way to learn data structures as a beginner programmer?\n",
    "Keywords: python, code, programming, error, syntax, function, debug, learn\n",
    "Topic Name: Beginner Programming and Debugging Help\n",
    "\n",
    "\n",
    "Topic:\n",
    "Sample texts from this topic:\n",
    "\n",
    "I meal prep every Sunday but by Wednesday I'm tired of eating the same thing.\n",
    "How do you make healthy eating sustainable when you have a busy schedule?\n",
    "I want to eat better but healthy food is so expensive compared to fast food.\n",
    "Keywords: food, healthy, diet, meal, eating, nutrition, cook, recipe\n",
    "Topic Name: Healthy Eating Habits and Meal Planning\n",
    "###REAL DATA###\n",
    "\n",
    "\n",
    "Topic:\n",
    "Sample texts from this topic:\n",
    "[DOCUMENTS]\n",
    "Keywords: [KEYWORDS]\n",
    "!!!Output only the topic name here. No explanations. No preamble. Just the topic name in English:\n",
    "\"\"\"\n",
    "\n",
    "# Set up the OpenAI client to point to your local Ollama server\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\" # required, but not used\n",
    ")\n",
    "\n",
    "# Create the representation model\n",
    "representation_model = OpenAI(\n",
    "    client,\n",
    "    model=\"qwen3:1.7b\", # Or your preferred model\n",
    "    prompt=prompt,\n",
    "    chat=True,\n",
    "    #delay_in_seconds=2 # To avoid rate-limiting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0536c",
   "metadata": {},
   "source": [
    "## Optional: Enhanced Topic Naming with Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27f8aff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 22:52:26,470 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-10-30 22:52:27,939 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-30 22:52:27,941 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-10-30 22:52:27,939 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-30 22:52:27,941 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-10-30 22:52:28,004 - BERTopic - Cluster - Completed ✓\n",
      "2025-10-30 22:52:28,004 - BERTopic - Cluster - Completed ✓\n",
      "2025-10-30 22:52:28,010 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-10-30 22:52:28,010 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "100%|██████████| 14/14 [17:43<00:00, 75.99s/it]\n",
      "\n",
      "2025-10-30 23:10:12,435 - BERTopic - Representation - Completed ✓\n",
      "2025-10-30 23:10:12,435 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "#| label: train-model\n",
    "\n",
    "# Assemble the BERTopic model\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model=representation_model,\n",
    "    min_topic_size=MIN_TOPIC_SIZE,\n",
    "    top_n_words=50,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Train the model on our texts and embeddings\n",
    "topics, probs = topic_model.fit_transform(df[\"text_clean\"].tolist(), embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d83cd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Found Topics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>192</td>\n",
       "      <td>-1_</td>\n",
       "      <td>[]</td>\n",
       "      <td>[inauguración de la estación olleros ., países...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0_</td>\n",
       "      <td>[]</td>\n",
       "      <td>[глава мид норвегии эйде: украина имеет право ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1_</td>\n",
       "      <td>[]</td>\n",
       "      <td>[aparecen restos de chapopote en costas de pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>2_</td>\n",
       "      <td>[]</td>\n",
       "      <td>[empresário abriu conta conjunta com namorada ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>3_</td>\n",
       "      <td>[]</td>\n",
       "      <td>[لولو معاك في العيد.. تردد قناة لولو \"وناسة\" ب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>4_</td>\n",
       "      <td>[]</td>\n",
       "      <td>[biden campaign admits “very delicate moment” ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>5_</td>\n",
       "      <td>[]</td>\n",
       "      <td>[biden anuncia propuesta de acuerdo sobre el a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>6_</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ricciardo's focus on improving f1 performance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>7_</td>\n",
       "      <td>[]</td>\n",
       "      <td>[dakota johnson on set of rom-com materialists...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>8_</td>\n",
       "      <td>[]</td>\n",
       "      <td>[rattrapée par ses déficits, la france voit sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count Name Representation  \\\n",
       "0     -1    192  -1_             []   \n",
       "1      0     62   0_             []   \n",
       "2      1     36   1_             []   \n",
       "3      2     28   2_             []   \n",
       "4      3     27   3_             []   \n",
       "5      4     25   4_             []   \n",
       "6      5     18   5_             []   \n",
       "7      6     17   6_             []   \n",
       "8      7     17   7_             []   \n",
       "9      8     17   8_             []   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [inauguración de la estación olleros ., países...  \n",
       "1  [глава мид норвегии эйде: украина имеет право ...  \n",
       "2  [aparecen restos de chapopote en costas de pla...  \n",
       "3  [empresário abriu conta conjunta com namorada ...  \n",
       "4  [لولو معاك في العيد.. تردد قناة لولو \"وناسة\" ب...  \n",
       "5  [biden campaign admits “very delicate moment” ...  \n",
       "6  [biden anuncia propuesta de acuerdo sobre el a...  \n",
       "7  [ricciardo's focus on improving f1 performance...  \n",
       "8  [dakota johnson on set of rom-com materialists...  \n",
       "9  [rattrapée par ses déficits, la france voit sa...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| label: inspect-results\n",
    "\n",
    "# Get detailed information about each topic\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Create the primary results dataframe\n",
    "results_df = df.copy()\n",
    "results_df[\"Topic\"] = topics\n",
    "results_df[\"Topic_Probability\"] = np.max(probs, axis=1)\n",
    "\n",
    "# Map the generated topic names (e.g., \"-1_word1_word2\") to the results\n",
    "name_map = dict(zip(topic_info[\"Topic\"], topic_info[\"Name\"]))\n",
    "results_df[\"Topic_Name\"] = results_df[\"Topic\"].map(name_map)\n",
    "\n",
    "# Save the primary outputs\n",
    "topic_info.to_csv(f\"{OUTPUT_DIR}/topic_info.csv\", index=False)\n",
    "results_df.to_csv(f\"{OUTPUT_DIR}/posts_with_topics.csv\", index=False)\n",
    "\n",
    "print(\"Top 10 Found Topics:\")\n",
    "topic_info.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f55a3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: crosstab\n",
    "# Only run if there are labels in the data\n",
    "if \"label\" in results_df.columns and results_df[\"label\"].nunique() > 1:\n",
    "    crosstab = pd.crosstab(results_df[\"label\"], results_df[\"Topic_Name\"])\n",
    "    crosstab.to_csv(f\"{OUTPUT_DIR}/label_topic_crosstab.csv\")\n",
    "    print(\"Crosstab of Labels vs. Topics:\")\n",
    "    display(crosstab.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53d9248e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating and saving interactive visualizations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations saved in 'outputs/'\n"
     ]
    }
   ],
   "source": [
    "#| label: visualizations\n",
    "#| warning: false\n",
    "\n",
    "print(\"Generating and saving interactive visualizations...\")\n",
    "\n",
    "# Inter-topic distance map\n",
    "fig_topics = topic_model.visualize_topics()\n",
    "fig_topics.write_html(f\"{OUTPUT_DIR}/viz_topics.html\")\n",
    "\n",
    "# Keyword scores per topic\n",
    "fig_barchart = topic_model.visualize_barchart(top_n_topics=20, n_words=10)\n",
    "fig_barchart.write_html(f\"{OUTPUT_DIR}/viz_barchart.html\")\n",
    "\n",
    "# Document projection map\n",
    "fig_documents = topic_model.visualize_documents(\n",
    "    docs=df[\"full_text\"].tolist(),\n",
    "    embeddings=embeddings,\n",
    "    hide_annotations=True\n",
    ")\n",
    "fig_documents.write_html(f\"{OUTPUT_DIR}/viz_documents.html\")\n",
    "\n",
    "# Hierarchical clustering of topics\n",
    "try:\n",
    "    fig_hierarchy = topic_model.visualize_hierarchy()\n",
    "    fig_hierarchy.write_html(f\"{OUTPUT_DIR}/viz_hierarchy.html\")\n",
    "except ValueError:\n",
    "    print(\"Could not generate hierarchy plot (not enough topics for hierarchical reduction).\")\n",
    "\n",
    "print(f\"Visualizations saved in '{OUTPUT_DIR}/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac1fbf",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "* The dataset was very difficult to work with for two main reasons:\n",
    "    1. The BERTopic analysis was probaly more fitting with an analysis based on only articles written in english.\n",
    "    2. The sample size for the analysis created very long and slow cell running time in the notebook regarding the BERTopic analysis.\n",
    "\n",
    "* The hierachial model and topics visualization clearly shows 4 different groups/clusters. The main differences between the clusters are the dominant keywords, topics, and types of news stories they contain, reflecting different areas of focus in global news coverage. For precise details, we could inspect the top keywords or example articles for each cluster in the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
